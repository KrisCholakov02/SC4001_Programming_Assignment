{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# Question B2 (10 marks)\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T07:54:16.984317Z",
     "start_time": "2024-03-06T07:54:14.239607Z"
    },
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    "1.Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T07:55:34.643919Z",
     "start_time": "2024-03-06T07:55:34.450030Z"
    },
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# Dividing the dataset into train, validation and test sets by applying the given conditions\n",
    "train_df = df[df['year'] <= 2020]  # Training data includes entries from year 2020 and before\n",
    "test_df = df[df['year'] >= 2021]  # Test data includes entries from year 2021 and after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    "2.Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T08:19:08.270970Z",
     "start_time": "2024-03-06T08:06:57.298566Z"
    },
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sc4001_assignment/lib/python3.8/site-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:360: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:06<00:00, 197.87it/s, loss=1.84e+5, metrics={'r2': -1.2476}]\n",
      "epoch 2: 100%|██████████| 1366/1366 [00:06<00:00, 205.21it/s, loss=1.01e+5, metrics={'r2': 0.4758}]\n",
      "epoch 3: 100%|██████████| 1366/1366 [00:06<00:00, 205.43it/s, loss=7.96e+4, metrics={'r2': 0.6846}]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:06<00:00, 207.05it/s, loss=6.6e+4, metrics={'r2': 0.7979}] \n",
      "epoch 5: 100%|██████████| 1366/1366 [00:06<00:00, 202.77it/s, loss=6.13e+4, metrics={'r2': 0.8291}]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:06<00:00, 203.77it/s, loss=5.92e+4, metrics={'r2': 0.8423}]\n",
      "epoch 7: 100%|██████████| 1366/1366 [00:06<00:00, 199.56it/s, loss=5.79e+4, metrics={'r2': 0.8495}]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:06<00:00, 200.82it/s, loss=5.68e+4, metrics={'r2': 0.8552}]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:06<00:00, 208.91it/s, loss=5.58e+4, metrics={'r2': 0.8607}]\n",
      "epoch 10: 100%|██████████| 1366/1366 [00:06<00:00, 206.53it/s, loss=5.47e+4, metrics={'r2': 0.8663}]\n",
      "epoch 11: 100%|██████████| 1366/1366 [00:06<00:00, 203.48it/s, loss=5.38e+4, metrics={'r2': 0.8705}]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:06<00:00, 209.86it/s, loss=5.31e+4, metrics={'r2': 0.8741}]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:06<00:00, 199.54it/s, loss=5.2e+4, metrics={'r2': 0.8795}] \n",
      "epoch 14: 100%|██████████| 1366/1366 [00:06<00:00, 205.89it/s, loss=5.13e+4, metrics={'r2': 0.8822}]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:06<00:00, 209.21it/s, loss=5.09e+4, metrics={'r2': 0.8845}]\n",
      "epoch 16: 100%|██████████| 1366/1366 [00:06<00:00, 202.75it/s, loss=5.02e+4, metrics={'r2': 0.8874}]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:06<00:00, 204.40it/s, loss=4.98e+4, metrics={'r2': 0.8892}]\n",
      "epoch 18: 100%|██████████| 1366/1366 [00:06<00:00, 204.67it/s, loss=4.96e+4, metrics={'r2': 0.8903}]\n",
      "epoch 19: 100%|██████████| 1366/1366 [00:06<00:00, 205.89it/s, loss=4.93e+4, metrics={'r2': 0.8917}]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:06<00:00, 212.01it/s, loss=4.91e+4, metrics={'r2': 0.8923}]\n",
      "epoch 21: 100%|██████████| 1366/1366 [00:06<00:00, 205.77it/s, loss=4.88e+4, metrics={'r2': 0.8939}]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:06<00:00, 204.67it/s, loss=4.88e+4, metrics={'r2': 0.894}] \n",
      "epoch 23: 100%|██████████| 1366/1366 [00:06<00:00, 214.98it/s, loss=4.85e+4, metrics={'r2': 0.8953}]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:06<00:00, 206.13it/s, loss=4.85e+4, metrics={'r2': 0.8949}]\n",
      "epoch 25: 100%|██████████| 1366/1366 [00:06<00:00, 203.43it/s, loss=4.85e+4, metrics={'r2': 0.8952}]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:06<00:00, 204.82it/s, loss=4.84e+4, metrics={'r2': 0.8956}]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:06<00:00, 195.59it/s, loss=4.82e+4, metrics={'r2': 0.8965}]\n",
      "epoch 28: 100%|██████████| 1366/1366 [00:06<00:00, 206.61it/s, loss=4.81e+4, metrics={'r2': 0.8969}]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:06<00:00, 201.79it/s, loss=4.82e+4, metrics={'r2': 0.8963}]\n",
      "epoch 30: 100%|██████████| 1366/1366 [00:06<00:00, 202.34it/s, loss=4.79e+4, metrics={'r2': 0.8978}]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:07<00:00, 193.62it/s, loss=4.8e+4, metrics={'r2': 0.8976}] \n",
      "epoch 32: 100%|██████████| 1366/1366 [00:07<00:00, 193.94it/s, loss=4.8e+4, metrics={'r2': 0.8973}] \n",
      "epoch 33: 100%|██████████| 1366/1366 [00:06<00:00, 196.52it/s, loss=4.8e+4, metrics={'r2': 0.8972}] \n",
      "epoch 34: 100%|██████████| 1366/1366 [00:07<00:00, 192.08it/s, loss=4.79e+4, metrics={'r2': 0.8973}]\n",
      "epoch 35: 100%|██████████| 1366/1366 [00:06<00:00, 195.71it/s, loss=4.79e+4, metrics={'r2': 0.8976}]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:06<00:00, 196.69it/s, loss=4.77e+4, metrics={'r2': 0.8986}]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:07<00:00, 194.21it/s, loss=4.77e+4, metrics={'r2': 0.8985}]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:06<00:00, 196.32it/s, loss=4.76e+4, metrics={'r2': 0.899}] \n",
      "epoch 39: 100%|██████████| 1366/1366 [00:06<00:00, 196.21it/s, loss=4.77e+4, metrics={'r2': 0.8984}]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:06<00:00, 196.47it/s, loss=4.76e+4, metrics={'r2': 0.8988}]\n",
      "epoch 41: 100%|██████████| 1366/1366 [00:06<00:00, 195.87it/s, loss=4.76e+4, metrics={'r2': 0.8989}]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:07<00:00, 194.04it/s, loss=4.73e+4, metrics={'r2': 0.9}]   \n",
      "epoch 43: 100%|██████████| 1366/1366 [00:06<00:00, 195.52it/s, loss=4.74e+4, metrics={'r2': 0.8998}]\n",
      "epoch 44: 100%|██████████| 1366/1366 [00:06<00:00, 195.69it/s, loss=4.73e+4, metrics={'r2': 0.9003}]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:06<00:00, 196.02it/s, loss=4.73e+4, metrics={'r2': 0.9002}]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:06<00:00, 195.87it/s, loss=4.74e+4, metrics={'r2': 0.8995}]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:06<00:00, 199.02it/s, loss=4.72e+4, metrics={'r2': 0.9007}]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:06<00:00, 195.55it/s, loss=4.71e+4, metrics={'r2': 0.9009}]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:06<00:00, 197.39it/s, loss=4.71e+4, metrics={'r2': 0.9008}]\n",
      "epoch 50: 100%|██████████| 1366/1366 [00:06<00:00, 197.04it/s, loss=4.69e+4, metrics={'r2': 0.9019}]\n",
      "epoch 51: 100%|██████████| 1366/1366 [00:06<00:00, 196.08it/s, loss=4.69e+4, metrics={'r2': 0.9016}]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:06<00:00, 196.33it/s, loss=4.68e+4, metrics={'r2': 0.902}] \n",
      "epoch 53: 100%|██████████| 1366/1366 [00:06<00:00, 196.91it/s, loss=4.68e+4, metrics={'r2': 0.902}] \n",
      "epoch 54: 100%|██████████| 1366/1366 [00:06<00:00, 196.17it/s, loss=4.7e+4, metrics={'r2': 0.9013}] \n",
      "epoch 55: 100%|██████████| 1366/1366 [00:06<00:00, 195.96it/s, loss=4.68e+4, metrics={'r2': 0.9023}]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:06<00:00, 196.45it/s, loss=4.67e+4, metrics={'r2': 0.9025}]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:06<00:00, 197.86it/s, loss=4.69e+4, metrics={'r2': 0.9018}]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:06<00:00, 196.14it/s, loss=4.68e+4, metrics={'r2': 0.9021}]\n",
      "epoch 59: 100%|██████████| 1366/1366 [00:06<00:00, 196.54it/s, loss=4.69e+4, metrics={'r2': 0.9015}]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:07<00:00, 194.93it/s, loss=4.68e+4, metrics={'r2': 0.9019}]\n",
      "epoch 61: 100%|██████████| 1366/1366 [00:07<00:00, 195.09it/s, loss=4.67e+4, metrics={'r2': 0.9025}]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:06<00:00, 195.79it/s, loss=4.67e+4, metrics={'r2': 0.9026}]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:06<00:00, 197.82it/s, loss=4.68e+4, metrics={'r2': 0.902}] \n",
      "epoch 64: 100%|██████████| 1366/1366 [00:06<00:00, 198.35it/s, loss=4.65e+4, metrics={'r2': 0.9031}]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:06<00:00, 195.77it/s, loss=4.66e+4, metrics={'r2': 0.9029}]\n",
      "epoch 66: 100%|██████████| 1366/1366 [00:06<00:00, 196.82it/s, loss=4.66e+4, metrics={'r2': 0.9027}]\n",
      "epoch 67: 100%|██████████| 1366/1366 [00:06<00:00, 196.19it/s, loss=4.64e+4, metrics={'r2': 0.9036}]\n",
      "epoch 68: 100%|██████████| 1366/1366 [00:06<00:00, 198.27it/s, loss=4.65e+4, metrics={'r2': 0.903}] \n",
      "epoch 69: 100%|██████████| 1366/1366 [00:06<00:00, 197.22it/s, loss=4.66e+4, metrics={'r2': 0.9027}]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:06<00:00, 197.38it/s, loss=4.63e+4, metrics={'r2': 0.904}] \n",
      "epoch 71: 100%|██████████| 1366/1366 [00:06<00:00, 196.67it/s, loss=4.65e+4, metrics={'r2': 0.903}] \n",
      "epoch 72: 100%|██████████| 1366/1366 [00:06<00:00, 196.84it/s, loss=4.62e+4, metrics={'r2': 0.9042}]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:06<00:00, 196.72it/s, loss=4.63e+4, metrics={'r2': 0.9042}]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:06<00:00, 196.70it/s, loss=4.63e+4, metrics={'r2': 0.9041}]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:06<00:00, 199.10it/s, loss=4.63e+4, metrics={'r2': 0.9038}]\n",
      "epoch 76: 100%|██████████| 1366/1366 [00:06<00:00, 196.99it/s, loss=4.61e+4, metrics={'r2': 0.9047}]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:06<00:00, 197.94it/s, loss=4.62e+4, metrics={'r2': 0.9044}]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:06<00:00, 196.54it/s, loss=4.61e+4, metrics={'r2': 0.9049}]\n",
      "epoch 79: 100%|██████████| 1366/1366 [00:06<00:00, 198.15it/s, loss=4.63e+4, metrics={'r2': 0.9037}]\n",
      "epoch 80: 100%|██████████| 1366/1366 [00:06<00:00, 199.26it/s, loss=4.63e+4, metrics={'r2': 0.9039}]\n",
      "epoch 81: 100%|██████████| 1366/1366 [00:06<00:00, 197.51it/s, loss=4.62e+4, metrics={'r2': 0.9042}]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:06<00:00, 198.78it/s, loss=4.6e+4, metrics={'r2': 0.9049}] \n",
      "epoch 83: 100%|██████████| 1366/1366 [00:06<00:00, 196.64it/s, loss=4.61e+4, metrics={'r2': 0.9048}]\n",
      "epoch 84: 100%|██████████| 1366/1366 [00:06<00:00, 197.95it/s, loss=4.61e+4, metrics={'r2': 0.905}] \n",
      "epoch 85: 100%|██████████| 1366/1366 [00:06<00:00, 196.07it/s, loss=4.61e+4, metrics={'r2': 0.9048}]\n",
      "epoch 86: 100%|██████████| 1366/1366 [00:06<00:00, 197.52it/s, loss=4.63e+4, metrics={'r2': 0.9039}]\n",
      "epoch 87: 100%|██████████| 1366/1366 [00:06<00:00, 197.68it/s, loss=4.62e+4, metrics={'r2': 0.9042}]\n",
      "epoch 88: 100%|██████████| 1366/1366 [00:06<00:00, 197.50it/s, loss=4.58e+4, metrics={'r2': 0.9058}]\n",
      "epoch 89: 100%|██████████| 1366/1366 [00:06<00:00, 199.45it/s, loss=4.6e+4, metrics={'r2': 0.9051}] \n",
      "epoch 90: 100%|██████████| 1366/1366 [00:06<00:00, 199.83it/s, loss=4.6e+4, metrics={'r2': 0.9051}] \n",
      "epoch 91: 100%|██████████| 1366/1366 [00:06<00:00, 199.55it/s, loss=4.6e+4, metrics={'r2': 0.9051}] \n",
      "epoch 92: 100%|██████████| 1366/1366 [00:06<00:00, 196.20it/s, loss=4.6e+4, metrics={'r2': 0.905}]  \n",
      "epoch 93: 100%|██████████| 1366/1366 [00:06<00:00, 198.49it/s, loss=4.59e+4, metrics={'r2': 0.9058}]\n",
      "epoch 94: 100%|██████████| 1366/1366 [00:06<00:00, 201.75it/s, loss=4.59e+4, metrics={'r2': 0.9056}]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:06<00:00, 196.72it/s, loss=4.58e+4, metrics={'r2': 0.9059}]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:06<00:00, 197.28it/s, loss=4.58e+4, metrics={'r2': 0.9058}]\n",
      "epoch 97: 100%|██████████| 1366/1366 [00:06<00:00, 199.43it/s, loss=4.6e+4, metrics={'r2': 0.9051}] \n",
      "epoch 98: 100%|██████████| 1366/1366 [00:06<00:00, 197.93it/s, loss=4.58e+4, metrics={'r2': 0.9059}]\n",
      "epoch 99: 100%|██████████| 1366/1366 [00:06<00:00, 198.33it/s, loss=4.58e+4, metrics={'r2': 0.906}] \n",
      "epoch 100: 100%|██████████| 1366/1366 [00:06<00:00, 196.85it/s, loss=4.58e+4, metrics={'r2': 0.9059}]\n"
     ]
    }
   ],
   "source": [
    "# Define the target\n",
    "target = train_df['resale_price'].values\n",
    "\n",
    "# Column type variables from the assignment pdf file\n",
    "categorical_cols = ['month', 'town', 'flat_model_type', 'storey_range']  # Categorical columns\n",
    "continuous_cols = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']  # Continuous columns\n",
    "\n",
    "# Create the TabPreprocessor\n",
    "tab_preprocessor = TabPreprocessor(cat_embed_cols=categorical_cols, continuous_cols=continuous_cols)\n",
    "\n",
    "# Transform the training dataset\n",
    "X_tab = tab_preprocessor.fit_transform(train_df)\n",
    "\n",
    "# Create the TabMlp model with 2 linear layers in the MLP, with 200 and 100 neurons respectively\n",
    "tabmlp = TabMlp(\n",
    "    mlp_hidden_dims=[200, 100],  # 2 linear layers in the MLP, with 200 and 100 neurons respectively\n",
    "    column_idx=tab_preprocessor.column_idx,  # Column indices\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,  # Embedding input\n",
    "    continuous_cols=continuous_cols  # Continuous columns\n",
    ")\n",
    "\n",
    "# Create the WideDeep model\n",
    "model = WideDeep(deeptabular=tabmlp)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # Pass the model\n",
    "    cost_function=\"rmse\",  # RMSE cost function\n",
    "    metrics=[R2Score()],  # R2 score\n",
    "    num_workers=0  # Set the num_workers parameter to 0\n",
    ")\n",
    "\n",
    "# Define the epochs and batch size\n",
    "no_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(\n",
    "    X_tab=X_tab,  # Pass the transformed training dataset\n",
    "    target=target,  # Target variable\n",
    "    n_epochs=no_epochs,  # Number of epochs\n",
    "    batch_size=batch_size  # Batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T08:23:55.812489Z",
     "start_time": "2024-03-06T08:23:53.401727Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:02<00:00, 519.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([173035.72, 192246.64, 289562.8 , ..., 594494.4 , 518723.94,\n",
       "       553011.7 ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test dataset\n",
    "X_tab_test = tab_preprocessor.transform(test_df)\n",
    "\n",
    "# Predict the target variable\n",
    "y_pred = trainer.predict(X_tab=X_tab_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    "3.Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-06T08:24:26.465818Z",
     "start_time": "2024-03-06T08:24:26.460127Z"
    },
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE & R2\n",
      "Test RMSE: 100703.4402070674\n",
      "Test R2: 0.6456869534456644\n"
     ]
    }
   ],
   "source": [
    "# Import the dependencies we will need to compute the RMSE and R2\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the ground truth and the predictions\n",
    "y_true = test_df['resale_price']  # Ground truth\n",
    "\n",
    "print('RMSE & R2')\n",
    "\n",
    "# Compute the RMSE\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)  # Set squared=False to get the RMSE\n",
    "print(f'Test RMSE: {rmse}')\n",
    "\n",
    "# Compute the R2 value\n",
    "r2 = r2_score(y_true, y_pred) \n",
    "print(f'Test R2: {r2}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
